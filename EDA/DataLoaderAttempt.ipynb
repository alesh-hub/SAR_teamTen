{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning\n",
    "import lightning_utilities\n",
    "import torchmetrics\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning Version: {pytorch_lightning.__version__}\")\n",
    "print(f\"Lightning Utilities Version: {lightning_utilities.__version__}\")\n",
    "print(f\"Torchmetrics Version: {torchmetrics.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "```python\n",
    "class MNISTDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"./\"):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\":\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(\n",
    "                mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n",
    "            )\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\":\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=32)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=32)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=32)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.mnist_predict, batch_size=32)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning import LightningDataModule\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "class SARDataModule(LightningDataModule):\n",
    "    \n",
    "    def __init__(self, data_dir: str = \"./\", batch_size: int = 8, val_split: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "\n",
    "        # Transformation for images\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Lambda(lambda x: x[0, :, :].unsqueeze(0))  # Take the first channel\n",
    "            # Augmentation TO DO\n",
    "        ])\n",
    "\n",
    "        # Transformation for masks\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "            # No channel selection for masks\n",
    "        ])\n",
    "        \n",
    "    def prepare_data(self) -> None:\n",
    "        # Not needed in our case, no download or labelling needed\n",
    "        # prepare_data(self) is used for operations that run only once and on one process.\n",
    "        pass \n",
    "\n",
    "        \n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        \n",
    "        # Helper function to get sorted file names\n",
    "        def get_sorted_file_names(folder_path):\n",
    "            return sorted([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "\n",
    "        # ----------------- TRAIN ----------------- \n",
    "        # Paths to the images and masks directories\n",
    "        train_images_dir = os.path.join(self.data_dir, 'train/images')\n",
    "        train_masks_dir = os.path.join(self.data_dir, 'train/labels_1D')\n",
    "\n",
    "        # Get the list of image filenames\n",
    "        train_images_filenames = get_sorted_file_names(train_images_dir)\n",
    "\n",
    "        # Generate full paths for images and masks\n",
    "        train_images_paths = [os.path.join(train_images_dir, f) for f in train_images_filenames]\n",
    "        train_masks_paths = [os.path.join(train_masks_dir, os.path.splitext(f)[0] + '.png') for f in train_images_filenames]\n",
    "\n",
    "        # Split into train and validation sets\n",
    "        train_images_paths, val_images_paths, train_masks_paths, val_masks_paths = train_test_split(\n",
    "            train_images_paths, train_masks_paths, test_size=self.val_split, random_state=42)\n",
    "        \n",
    "        # ----------------- TEST ----------------- \n",
    "        # Paths to the test dataset\n",
    "        test_images_dir = os.path.join(self.data_dir, 'test/images')\n",
    "        test_masks_dir = os.path.join(self.data_dir, 'test/labels_1D')\n",
    "\n",
    "        # Get the list of test image filenames\n",
    "        test_images_filenames = get_sorted_file_names(test_images_dir)\n",
    "\n",
    "        # Generate full paths for test images and masks\n",
    "        test_images_paths = [os.path.join(test_images_dir, f) for f in test_images_filenames]\n",
    "        test_masks_paths = [os.path.join(test_masks_dir, os.path.splitext(f)[0] + '.png') for f in test_images_filenames]\n",
    "\n",
    "        # ----------------- LOADING ----------------- \n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = SARImageDataset(\n",
    "                train_images_paths, train_masks_paths,\n",
    "                image_transform=self.image_transform, mask_transform=self.mask_transform\n",
    "            )\n",
    "            self.val_dataset = SARImageDataset(\n",
    "                val_images_paths, val_masks_paths,\n",
    "                image_transform=self.image_transform, mask_transform=self.mask_transform\n",
    "            )\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = SARImageDataset(\n",
    "                test_images_paths, test_masks_paths,\n",
    "                image_transform=self.image_transform, mask_transform=self.mask_transform\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "class SARImageDataset(Dataset): # Allows the user to apply a custom transformation via self.mask_transform. \n",
    "\n",
    "    def __init__(self, images_paths, masks_paths, image_transform=None, mask_transform=None):\n",
    "        self.images_paths = images_paths\n",
    "        self.masks_paths = masks_paths\n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        image_path = self.images_paths[idx]\n",
    "        with Image.open(image_path) as img:\n",
    "            if self.image_transform:\n",
    "                img = self.image_transform(img)\n",
    "            else:\n",
    "                img = transforms.ToTensor()(img)\n",
    "\n",
    "        # Load the corresponding mask\n",
    "        mask_path = self.masks_paths[idx]\n",
    "        if not os.path.exists(mask_path):\n",
    "            raise FileNotFoundError(f\"Mask file not found: {mask_path}\")\n",
    "        with Image.open(mask_path) as mask:\n",
    "            # Convert mask to grayscale (DO WE NEED IT?)\n",
    "            # mask = mask.convert('L')\n",
    "            if self.mask_transform:\n",
    "                mask = self.mask_transform(mask)\n",
    "            else:\n",
    "                mask = transforms.ToTensor()(mask)\n",
    "            mask = mask.squeeze(0).long()              \n",
    "\n",
    "        # Return both the image and its corresponding mask\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 2\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 3\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 4\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 5\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 6\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 7\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 8\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 9\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 10\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 11\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 12\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 13\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 14\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 15\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 16\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 17\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 18\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 19\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 20\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 21\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 22\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 23\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 24\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 25\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 26\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 27\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 28\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 29\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 30\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 31\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 32\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 33\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 34\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 35\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 36\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 37\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 38\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 39\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 40\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 41\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 42\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 43\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 44\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 45\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 46\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 47\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 48\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 49\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 50\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 51\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 52\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 53\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 54\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 55\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 56\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 57\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 58\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 59\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 60\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 61\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 62\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 63\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 64\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 65\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 66\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 67\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 68\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 69\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 70\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 71\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 72\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 73\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 74\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 75\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 76\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 77\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 78\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 79\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 80\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 81\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 82\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 83\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 84\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 85\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 86\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 87\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 88\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 89\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 90\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 91\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 92\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 93\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 94\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 95\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 96\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 97\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 98\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 99\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 100\n",
      "Images batch shape: torch.Size([8, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([8, 650, 1250])\n",
      "---\n",
      "Batch 101\n",
      "Images batch shape: torch.Size([1, 3, 650, 1250])\n",
      "Masks batch shape: torch.Size([1, 650, 1250])\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "data_module = SARDataModule(data_dir=\"../dataset\", batch_size=8, val_split=0.2)\n",
    "data_module.prepare_data()\n",
    "data_module.setup('fit')\n",
    "\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f\"Batch {i+1}\")\n",
    "    print(f\"Images batch shape: {batch[0].shape}\")\n",
    "    \n",
    "    print(f\"Masks batch shape: {batch[1].shape}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Lightning Version: 2.4.0\n",
      "Lightning Utilities Version: 0.11.8\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning\n",
    "import lightning_utilities\n",
    "#import torch\n",
    "print(f\"PyTorch Lightning Version: {pytorch_lightning.__version__}\")\n",
    "print(f\"Lightning Utilities Version: {lightning_utilities.__version__}\")\n",
    "#torch.set_float32_matmul_precision('medium' | 'high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large, DeepLabV3_MobileNet_V3_Large_Weights\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "\n",
    "class SARSegmentationModel(LightningModule):\n",
    "    def __init__(self, learning_rate=5e-5, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = deeplabv3_mobilenet_v3_large(weights=DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT)\n",
    "        self.model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.train_iou = torchmetrics.JaccardIndex(num_classes=num_classes, task=\"multiclass\")\n",
    "        self.val_iou = torchmetrics.JaccardIndex(num_classes=num_classes, task=\"multiclass\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)[\"out\"]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, masks.long())\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        iou = self.train_iou(preds, masks)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_iou\", iou, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, masks.long())\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        iou = self.val_iou(preds, masks)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_iou\", iou, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "# Assuming SARDataModule is defined elsewhere and works correctly\n",
    "data_module = SARDataModule(data_dir=\"../dataset\", batch_size=8, val_split=0.2)\n",
    "\n",
    "model = SARSegmentationModel(learning_rate=5e-5, num_classes=5)\n",
    "\n",
    "trainer = Trainer(max_epochs=10, devices=1, accelerator=\"gpu\")\n",
    "trainer.fit(model, datamodule=data_module)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
